# Necessary preparation for the plotting
# install necessary packages
# install.packages("install.load")
# # might be required
devtools::install_github('tidyverse/ggplot2')
library(install.load)
packages <-
c(
"knitr",
"quanteda",
"tidyverse",
"tidytext",
"reshape2",
"ggplot2",
"dplyr",
"stringr",
"wordcloud",
"SnowballC",
"lubridate",
"plotly",
"knitr",
"wesanderson",
"ggpubr",
"rmarkdown"
)
install_load(packages)
# read in csv
fulldata <- read.csv("~/Dropbox/Big Data/fulldata.csv", stringsAsFactors=FALSE)
descriptives <- read.csv("~/Dropbox/Big Data/descriptives.csv", stringsAsFactors=FALSE)
# for color in plots
cbp1 <- c("#999999", "#E69F00", "#56B4E9", "#009E73",
"#F0E442", "#0072B2", "#D55E00", "#CC79A7")
# Prepare the corpus
comments <- fulldata %>%
dplyr::group_by(group) %>%
mutate(line = row_number(),
comment = cumsum(str_detect(
textDisplay, regex("^chapter [\\divxlc]",
ignore_case = TRUE)
))) %>%
ungroup()
tidy_comments <- comments %>%
unnest_tokens(word, textDisplay)
# stemming
tidy_comments <- tidy_comments %>%
dplyr::mutate_at("word", list(~wordStem((.), language = "en")))
# remove white space
tidy_comments$word <- gsub("\\s+", "", tidy_comments$word)
# remove numbers
tidy_comments <-
tidy_comments[-grep("\\b\\d+\\b", tidy_comments$word),]
# tidytext automatically lowers and removes punctuation
# remove stopwords
cleaned_comments <- tidy_comments %>%
anti_join(stop_words, by = c("word" = "word")) %>%
filter(!str_detect(word, "â"),
!str_detect(word, "ðÿ"),
!str_detect(word, "trump"))
# Sentiment analysis
sentiment_by_time <- cleaned_comments %>%
# Define a new column using floor_date()
mutate(date = floor_date(as.Date(publishedAt), unit = "week")) %>%
# Group by date
group_by(group, date) %>%
mutate(total_words = n()) %>%
ungroup() %>%
# Implement sentiment analysis using the NRC lexicon
inner_join(get_sentiments("bing"))
# Necessary preparation for the plotting
# install necessary packages
# install.packages("install.load")
# # might be required
devtools::install_github('tidyverse/ggplot2')
library(install.load)
packages <-
c(
"knitr",
"quanteda",
"tidyverse",
"tidytext",
"reshape2",
"ggplot2",
"dplyr",
"stringr",
"wordcloud",
"SnowballC",
"lubridate",
"plotly",
"knitr",
"wesanderson",
"ggpubr",
"rmarkdown"
)
install_load(packages)
# read in csv
fulldata <- read.csv("~/Dropbox/Big Data/fulldata.csv", stringsAsFactors=FALSE)
descriptives <- read.csv("~/Dropbox/Big Data/descriptives.csv", stringsAsFactors=FALSE)
# for color in plots
cbp1 <- c("#999999", "#E69F00", "#56B4E9", "#009E73",
"#F0E442", "#0072B2", "#D55E00", "#CC79A7")
# Prepare the corpus
comments <- fulldata %>%
dplyr::group_by(group) %>%
mutate(line = row_number(),
comment = cumsum(str_detect(
textDisplay, regex("^chapter [\\divxlc]",
ignore_case = TRUE)
))) %>%
ungroup()
tidy_comments <- comments %>%
unnest_tokens(word, textDisplay)
# stemming
tidy_comments <- tidy_comments %>%
dplyr::mutate_at("word", list(~wordStem((.), language = "en")))
# remove white space
tidy_comments$word <- gsub("\\s+", "", tidy_comments$word)
# remove numbers
tidy_comments <-
tidy_comments[-grep("\\b\\d+\\b", tidy_comments$word),]
# tidytext automatically lowers and removes punctuation
# remove stopwords
cleaned_comments <- tidy_comments %>%
anti_join(stop_words, by = c("word" = "word")) %>%
filter(!str_detect(word, "â"),
!str_detect(word, "ðÿ"),
!str_detect(word, "trump"))
# Sentiment analysis
sentiment_by_time <- cleaned_comments %>%
# Define a new column using floor_date()
mutate(date = floor_date(as.Date(publishedAt), unit = "week")) %>%
# Group by date
group_by(group, date) %>%
mutate(total_words = n()) %>%
ungroup() %>%
# Implement sentiment analysis using the NRC lexicon
inner_join(get_sentiments("bing"))
